{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data ingestion  <a name=\"data_ingestion\"></a> \n",
    " \n",
    "## Ingesting data from Redshift cluster\n",
    " \n",
    "### Step 1) Create Redshift connector in Hopsworks\n",
    " \n",
    "Look at the \"Get started with Redshift and the Feature Store\" notebook for a step by step guide on how to create a redshift cluster. \n",
    " \n",
    "#### Create a redshift connector in Hopsworks for your redshift cluster.\n",
    "![create-connector.png](images/redshift/create-connector.png)\n",
    " \n",
    "Enter a unique name for your connector and in the Refshift tab fill in the cluster identifier, database driver name, endpoint, database name, database port, and database user fields.\n",
    "You can use a password or an IAM role to connect to the database. If you use an IAM role a temporary password will be generated for the user every time you get the connector. \n",
    "The IAM role needs a policy that will allow it to get temporary credentials for the user. An example policy that will allow GetClusterCredentials, CreateClusterUser, and JoinGroup on database `dev` is shown below.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowRedshiftTempCreds\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"redshift:GetClusterCredentials\",\n",
    "                \"redshift:CreateClusterUser\",\n",
    "                \"redshift:JoinGroup\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:redshift:us-east-2:123456789011:dbuser:redshift-cluster-1/temp_creds_user\",\n",
    "                \"arn:aws:redshift:us-east-2:123456789011:dbname:redshift-cluster-1/dev\",\n",
    "                \"arn:aws:redshift:us-east-2:123456789011:dbgroup:redshift-cluster-1/auto_login_group\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "A temporary password is only valid for 1 hour.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Get a storage connector by name"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()\n",
    "# Get the connector created above \n",
    "connector_redshift = fs.get_storage_connector(\"connector-1\", \"REDSHIFT\")\n",
    "options = connector_redshift.spark_options()"
   ]
  },
  {
   "source": [
    "### Create a table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n",
    ".csv(\"hdfs:///Projects/redshift/redshift_Training_Datasets/telco_customer_churn.csv\")\n",
    "\n",
    "telcom.write.\\\n",
    "format(\"jdbc\").options(**options).\\\n",
    "mode(\"overwrite\").\\\n",
    "save()"
   ]
  },
  {
   "source": [
    "### Read data from a table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcom = spark.read.format(\"jdbc\").options(**options).load()\n",
    "telcom.show(10)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}